<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>Chapter 20: 附录 B：OpenCC、脚本映射与正则工具箱（深度实战版）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">ASR 与 Speaker Diarization 训练中文教程（多语种：中英及主要语种｜RNN → Conv+LSTM → MLLM）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 1: 任务全景：ASR 与 Diarization 的训练对象、边界与通用流水线</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 2: 工程与实验基线：环境、框架、分布式与可复现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 3: 数据与标注：采集、清洗、切分、对齐与许可</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 4: 文本规范化全家桶：TN / ITN / OpenCC / 混语与混脚本</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 5: 音频预处理与切分：VAD、重叠、对齐与波形级增广</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 6: 特征与前端：从 MFCC 到可学习前端，再到 SSL 表征</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 7: RNN 时代 ASR：从 LSTM/GRU 到 CTC/Attention（并讨论对 MLLM 的启示）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 8：Conv + LSTM 时代：CLDNN/CRDNN/TDNN-LSTM 与流式工程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 9: Transformer 自监督过渡：Conformer、Transducer 与 SSL 微调</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 10: Speaker Diarization 经典流水线：SAD + Embedding + Clustering + Resegmentation</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 11: 神经 Diarization 与端到端联合：EEND、TS-VAD、SA-ASR</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 12: 多语种与混语训练 (Multilingual & Code-Switching)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 13: 评测与误差分析——ASR (WER/CER/MER) 与 Diarization (DER/JER) 的细节陷阱</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 14: 开源工具链与训练配方：Kaldi / ESPnet / NeMo / WeNet / FunASR / Pyannote</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 15: 开源数据集大全：ASR / 多语种 / 会议 / 噪声 / Diarization</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 16: MLLM 时代：从 Speech Foundation Model 到“可对话的语音智能体”</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 17: MLLM 新内容：RAG 热词识别、上下文增强与说话人知识注入</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 18: 生产化落地：流式、延迟、部署、监控、隐私与安全</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 19: 附录 A：TN / ITN 速查与工程实战手册</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 20: 附录 B：OpenCC、脚本映射与正则工具箱（深度实战版）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 21: 附录 C - 术语表、常见问答 (FAQ) 与进阶阅读</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="README.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">ASR 与 Speaker Diarization 训练中文教程（多语种：中英及主要语种｜RNN → Conv+LSTM → MLLM）</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="chapter-20-bopencc">Chapter 20: 附录 B：OpenCC、脚本映射与正则工具箱（深度实战版）</h1>
<h2 id="201">20.1 开篇段落</h2>
<p>在 ASR（自动语音识别）和 Speaker Diarization（说话人日志）的训练与评测中，文本处理的“卫生状况”往往决定了模型的上限。数据清洗不仅仅是“去掉空格”那么简单，它涉及字符编码的底层原理、语言学的正字法（Orthography）规范以及多语种混合时的冲突治理。</p>
<p><strong>为什么这一章至关重要？</strong></p>
<ol>
<li><strong>Tokenizer 爆炸</strong>：未规范化的文本会导致 BPE/SentencePiece 词表充斥着意义相同的冗余 Token（如全角/半角数字、异体字），稀释了每个 Token 的训练数据量。</li>
<li><strong>评测虚高/虚低</strong>：如果 Ground Truth 是“OK”，而模型输出“okay”，在未做归一化的情况下会被判错，导致 WER 无法反映真实听感效果。</li>
<li><strong>MLLM 的幻觉诱因</strong>：对于多模态大模型，不一致的文本输入（如混用的繁简字）会作为一种“隐性提示”，诱导模型输出错误的方言词汇或产生幻觉。</li>
</ol>
<p>本章将提供一套工业级的处理标准，涵盖 OpenCC 的高级用法、CJK（中日韩）脚本冲突的深度治理、粤语混写的标准化方案，以及 Unicode 字符层的“排雷”指南。</p>
<hr />
<h2 id="202-opencc">20.2 OpenCC 进阶配置与字典干预</h2>
<p>OpenCC（Open Chinese Convert）是处理中文繁简转换的事实标准。在 ASR 领域，我们不仅要关注“字”的转换，更要关注“词”与“义”的对齐。</p>
<h3 id="2021">20.2.1 配置文件深度解析与选型</h3>
<p>OpenCC 的配置文件本质上是多个词典（Dictionary）与转换链（Segmentation）的组合。</p>
<p>| 配置文件 (<code>.json</code>) | 转换逻辑 | ASR 场景适用性 | 深度解析 |</p>
<table>
<thead>
<tr>
<th>配置文件 (<code>.json</code>)</th>
<th>转换逻辑</th>
<th>ASR 场景适用性</th>
<th>深度解析</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>s2t.json</code></td>
<td>简 → 繁 (字符)</td>
<td><strong>极低</strong></td>
<td>仅做 Codepoint 映射。会将“后天”转为“后天”（正确应为“後天”），“皇后”转为“皇後”（正确应为“皇后”）。<strong>严禁在 ASR 语料处理中使用</strong>，否则会制造大量语义噪音。</td>
</tr>
<tr>
<td><code>s2twp.json</code></td>
<td>简 → 台湾正体 (含短语)</td>
<td><strong>推荐 (台湾场景)</strong></td>
<td>包含“短语映射”。如：<code>计算机</code> → <code>電腦</code>，<code>出租车</code> → <code>計程車</code>。这对于训练面向台湾口音的 ASR 至关重要，因为 LM 需要学习当地的用词概率。</td>
</tr>
<tr>
<td><code>t2s.json</code></td>
<td>繁 → 简 (字符)</td>
<td><strong>中 (通用场景)</strong></td>
<td>适合将繁体语料并入大规模简体训练集。注意：它会将“電腦”转回“电脑”，这在做混合语种训练时是可接受的。</td>
</tr>
<tr>
<td><code>tw2sp.json</code></td>
<td>台湾正体 → 简 (含短语)</td>
<td><strong>高 (回译增强)</strong></td>
<td>当你需要利用台湾语料（如 PTT 语音、台剧）来增强陆普通话模型时，此配置能将台湾特有词汇“翻译”为大陆习惯，减少 OOV。</td>
</tr>
<tr>
<td><code>hk2s.json</code></td>
<td>香港繁体 → 简</td>
<td><strong>中 (粤语辅助)</strong></td>
<td>处理香港特有汉字（如“裏”转“里”，“羣”转“群”）。</td>
</tr>
</tbody>
</table>
<h3 id="2022">20.2.2 “防误伤”机制：自定义词典与白名单</h3>
<p>OpenCC 的短语转换有时会“过度纠正”，特别是涉及人名、地名或专有名词时。</p>
<ul>
<li><strong>问题案例</strong>：</li>
<li>人名“周杰伦”在 <code>s2twp</code> 下通常正常，但某些生僻人名或品牌名可能会被错误地“本地化”。</li>
<li>
<p>例如：某品牌名包含“内存”，在转台湾正体时变成了“記憶體”，但该品牌本身就是叫“XX内存”。</p>
</li>
<li>
<p><strong>解决方案：占位符保护法 (Placeholder Protection)</strong>
在调用 OpenCC 之前，执行以下逻辑：</p>
</li>
</ul>
<ol>
<li><strong>扫描</strong>：利用 AC 自动机或正则，匹配白名单词汇（如专有名词表）。</li>
<li><strong>替换</strong>：将命中词汇替换为唯一哈希值或特殊 Token（如 <code>__PROTECTED_001__</code>）。</li>
<li><strong>转换</strong>：运行 OpenCC。</li>
<li><strong>还原</strong>：将特殊 Token 替换回原文。</li>
</ol>
<h3 id="2023">20.2.3 异体字标准化的“幂等性”</h3>
<p>在数据清洗流水线中，<strong>幂等性（Idempotency）</strong> 是关键——即处理一次和处理十次，结果应该一样。</p>
<ul>
<li><strong>常见陷阱</strong>：繁体字内部存在大量异体字（如“真”与“眞”，“为”与“爲”）。</li>
<li><strong>最佳实践</strong>：在所有繁体数据进入训练前，强制运行一次 <code>t2t.json</code>（繁体到繁体标准字）。OpenCC 默认倾向于将异体字归一化为通用字形，这能显著降低 Character-level 的词表大小（Vocab Size），提升模型收敛速度。</li>
</ul>
<hr />
<h2 id="203-cjk">20.3 CJK 脚本冲突与字符集治理 (中日韩混同问题)</h2>
<p>ASR 模型最怕的就是“看着一样，读音不同”。Unicode 的 <strong>Han Unification (汉字统合)</strong> 给多语种 ASR 带来了巨大挑战。</p>
<h3 id="2031-han-unification">20.3.1 汉字统合灾难 (Han Unification)</h3>
<p>Unicode 将中文、日文、韩文中起源相同、字形相似的字符映射到了一个码位（Code Point）。</p>
<ul>
<li><strong>典型冲突</strong>：</li>
<li><code>直</code> (U+76F4): 中文 <code>zhí</code>, 日文 <code>choku/nao</code>。</li>
<li>
<p><code>骨</code> (U+9AA8): 中文 <code>gǔ</code>, 日文 <code>kotsu/hone</code> (字形写法在不同字体下甚至不同)。</p>
</li>
<li>
<p><strong>对 ASR 的影响</strong>：如果训练数据混合了中日文且未加区分，Shared Encoder 会在该 Token 上看到两组完全不同的声学分布（Acoustic Distribution）。结果是模型在解码时出现“犹豫”，导致置信度降低或输出乱码。</p>
</li>
</ul>
<h3 id="2032-lid-token">20.3.2 解决方案：LID Token 与 脚本提示</h3>
<ol>
<li>
<p><strong>LID (Language ID) Prefix</strong>：
* 数据格式：<code>&lt;|zh|&gt; 今天天气不错</code> vs <code>&lt;|ja|&gt; 今日はいい天気だ</code>。
* <strong>原理</strong>：利用 Transformer 的 Attention 机制，将语种信息注入到整个序列的上下文中，使模型针对同一个 <code>直</code> 字根据 Prefix 激活不同的声学特征提取路径。</p>
</li>
<li>
<p><strong>Prompt 约束 (针对 MLLM)</strong>：
* 在 Instruction Tuning 阶段，明确要求输出脚本。
* Prompt: <code>Transcribe the audio into Japanese script (Kanji/Kana).</code>
* 这能抑制模型输出简体中文特有汉字的倾向。</p>
</li>
</ol>
<h3 id="2033-kana">20.3.3 日文假名 (Kana) 的特殊清洗</h3>
<p>日语书写系统极其复杂，以下清理步骤是<strong>必须</strong>的：</p>
<ol>
<li>
<p><strong>半角片假名 (Half-width Katakana) → 全角</strong>：
* <code>ｱ</code> (U+FF71) → <code>ア</code> (U+30A2)。
* 原因：半角假名在 Tokenizer 中往往被切分得很碎，且在视觉上不美观。
* <strong>正则范围</strong>：<code>[\uFF61-\uFF9F]</code> (半角假名区域)。</p>
</li>
<li>
<p><strong>浊音与半浊音的 NFD 问题 (macOS/Linux 差异)</strong>：
* <strong>NFC (Precomposed)</strong>: <code>が</code> (U+304C) - 推荐。
* <strong>NFD (Decomposed)</strong>: <code>か</code> (U+304B) + <code>゙</code> (Combining Dakuten U+3099)。
* <strong>陷阱</strong>：macOS 的文件系统倾向于使用 NFD。如果你的数据是在 Mac 上解压或处理的，可能会得到 NFD 形式。这会导致 Tokenizer 将 <code>が</code> 切分为两个 Token，严重破坏对齐。
* <strong>Rule-of-Thumb</strong>：所有文本入库前，强制执行 <code>unicodedata.normalize('NFKC', text)</code>。</p>
</li>
<li>
<p><strong>长音符标准化</strong>：
* 日语长音符 <code>ー</code> (U+30FC) 极易与中文破折号 <code>—</code> (U+2014) 或 英文连字符 <code>-</code> (U+002D) 混淆。
* <strong>清洗逻辑</strong>：如果文本被标记为日语，且出现非 <code>U+30FC</code> 的横线符号，需根据上下文判断是否替换为长音符。</p>
</li>
</ol>
<hr />
<h2 id="204-cantonese">20.4 粤语 (Cantonese) 的“三态”混写治理</h2>
<p>粤语 ASR 的难点在于“写什么”没有绝对标准。</p>
<h3 id="2041">20.4.1 文本的三种形态</h3>
<ol>
<li><strong>书面语 (SW)</strong>: “他在这里” —— 读音可以是普通话，也可以是粤语读文（Reading pronunciation）。</li>
<li><strong>口语汉字 (Spoken Hanzi)</strong>: “佢喺度” —— 记录真实的粤语口语词汇。</li>
<li><strong>粤拼/火星文混合</strong>: “keoi hai dou”, “佢hai度”。</li>
</ol>
<h3 id="2042">20.4.2 混写治理流水线</h3>
<p>针对训练数据参差不齐的情况，建议采取以下标准化步骤：</p>
<ul>
<li><strong>Step 1: 英文/粤拼隔离</strong></li>
<li>检测句子中的拉丁字母。如果是有效的英文单词（查词典），保留（Code-switching）。</li>
<li>
<p>如果是无意义串或类粤拼，尝试通过 WFST 或查表法转回汉字（慎用，错误率高）。如果无法恢复，建议丢弃该句。</p>
</li>
<li>
<p><strong>Step 2: 粤语特有字标准化 (Orthography)</strong>
粤语有很多同音异字写法，需强制统一：</p>
</li>
<li>
<p><code>既</code> / <code>嘅</code> → 区分用法：<code>嘅</code> (Possessive/Particle, 的), <code>既</code> (Since/Already)。若无法区分，ASR 训练通常统一为 <code>嘅</code>（因为更高频）。</p>
</li>
<li><code>D</code> / <code>啲</code> → 统一为 <code>啲</code>。</li>
<li>
<p><code>地</code> / <code>哋</code> → 复数人称统一为 <code>哋</code>（如：我哋）。</p>
</li>
<li>
<p><strong>Step 3: 语气助词 (Final Particles) 处理</strong></p>
</li>
<li>粤语包含大量语气词：la1, wo3, ge3, zek1。</li>
<li><strong>策略</strong>：如果目标是字幕输出，通常保留对应的汉字（啦、沃、葛、杰）。如果目标是语义理解，有时会训练模型输出特殊 Tag <code>&lt;|particle|&gt;</code> 或直接忽略，以减少 ASR 的插入错误（Insertion Error）。</li>
</ul>
<hr />
<h2 id="205-unicode-regex-toolkit">20.5 Unicode 清洗与正则工具箱 (Regex Toolkit)</h2>
<p>这是数据工程师的“瑞士军刀”。</p>
<h3 id="2051-unicode-normalization-forms-nfc-vs-nfd">20.5.1 Unicode Normalization Forms (NFC vs NFD)</h3>
<p>这是最容易被忽视的 Bug 头。</p>
<ul>
<li><strong>NFC (Normalization Form Composition)</strong>: 字符尽可能合并。<code>e</code> + <code>´</code> → <code>é</code>。 <strong>(ASR 标准)</strong></li>
<li><strong>NFD (Normalization Form Decomposition)</strong>: 字符尽可能拆分。<code>é</code> → <code>e</code> + <code>´</code>。</li>
<li><strong>NFKC</strong>: 在 NFC 基础上，处理兼容字符（全角变半角，<code>④</code> 变 <code>4</code>）。</li>
</ul>
<blockquote>
<p><strong>Gotcha</strong>: 某些开源数据集（特别是来自老旧系统或 Mac 处理过的）可能是 NFD 格式。如果不转 NFKC，你的词表里会出现裸露的音标符号，导致训练发散。</p>
</blockquote>
<h3 id="2052-regex-recipes">20.5.2 常用正则逻辑表 (Regex Recipes)</h3>
<p>以下正则逻辑假设环境为 Python <code>re</code> 模块。</p>
<p>| 目标场景 | 正则逻辑描述 | 示例与说明 |</p>
<table>
<thead>
<tr>
<th>目标场景</th>
<th>正则逻辑描述</th>
<th>示例与说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>时间戳移除</strong></td>
<td>匹配 <code>\[\d{2}:\d{2}(\.\d{2,3})?\]</code> 或 <code>&lt;.*?&gt;</code></td>
<td>字幕文件常见 <code>[00:12.50]</code> 或 XML 标签，必须彻底清除。</td>
</tr>
<tr>
<td><strong>不可见字符</strong></td>
<td>匹配 <code>[\x00-\x1f\x7f\u200b-\u200f\u2028-\u202f]</code></td>
<td>包含 ASCII 控制符、零宽空格 (ZWSP)、行分隔符等。ASR 杀手，必须删。</td>
</tr>
<tr>
<td><strong>括号内容清洗</strong></td>
<td>匹配 <code>\（.*?\）</code> 或 <code>\(.*?\)</code></td>
<td><strong>小心使用</strong>。如果是 <code>(笑声)</code> 可以删；如果是 <code>(IBM)</code> 则包含语义。建议改为：仅删除包含特定关键词（笑、掌声、背景音）的括号。</td>
</tr>
<tr>
<td><strong>多余标点</strong></td>
<td>匹配 <code>[^\w\s\u4e00-\u9fa5]</code> (反向逻辑)</td>
<td>仅保留字、数、空白。用于构建纯文本语料。</td>
</tr>
<tr>
<td><strong>重复标点</strong></td>
<td>匹配 <code>([。，！？])\1+</code> 替换为 <code>\1</code></td>
<td><code>真的吗？？！！！</code> → <code>真的吗？！</code> (或标准化为单个)。</td>
</tr>
<tr>
<td><strong>连续空格压缩</strong></td>
<td>匹配 <code>\s+</code> 替换为 <code></code> (Space)</td>
<td>防止 Tokenizer 将 <code>Space</code> 和 <code>Double Space</code> 视为不同 Token。</td>
</tr>
</tbody>
</table>
<h3 id="2053-python-pseudocode">20.5.3 脚本映射示例 (Python Pseudocode)</h3>
<p>处理中日同形字的简单映射逻辑：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">script_aware_normalization</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="p">):</span>
    <span class="c1"># 1. Unicode Normalize first</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFKC&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

    <span class="c1"># 2. Language specific rules</span>
    <span class="k">if</span> <span class="n">lang</span> <span class="o">==</span> <span class="s1">&#39;ja&#39;</span><span class="p">:</span>
        <span class="c1"># 强制长音符标准化</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;—&#39;</span><span class="p">,</span> <span class="s1">&#39;ー&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;ー&#39;</span><span class="p">)</span>
        <span class="c1"># 全角英数转半角 (NFKC已做，但可加固)</span>
        <span class="c1"># 假名标准化...</span>
    <span class="k">elif</span> <span class="n">lang</span> <span class="o">==</span> <span class="s1">&#39;zh&#39;</span><span class="p">:</span>
        <span class="c1"># 移除日文特有假名 (防止标注错误混入)</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[\u3040-\u30ff]&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Japanese Kana found in Chinese text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># 策略：丢弃样本 或 视为垃圾字符删除</span>

    <span class="c1"># 3. Whitespace cleanup</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text</span>
</code></pre></div>

<hr />
<h2 id="206-normalization-for-evaluation">20.6 评测前统一化 (Normalization for Evaluation)</h2>
<p>为了计算 WER/CER，必须制定一套不可动摇的规则（Protocol）。</p>
<h3 id="2061-english-protocol">20.6.1 英文评测标准 (English Protocol)</h3>
<p>通常遵循 <strong>Kaldi</strong> 或 <strong>Whisper</strong> 的归一化标准：</p>
<ol>
<li><strong>Lowercasing</strong>: 转小写。</li>
<li>
<p><strong>Punctuation Removal</strong>: 移除除 <code>'</code> (Apostrophe) 以外的所有标点。
* <em>注</em>：<code>don't</code> 保留为 <code>don't</code> 还是拆分为 <code>dont</code> 或 <code>do not</code>，需看 Tokenizer 训练时的决定。通常保留 <code>'</code>。</p>
</li>
<li>
<p><strong>Number Expansion (ITN)</strong>: 将 <code>20</code> 转为 <code>twenty</code> (推荐) 或反之。<strong>必须统一</strong>。</p>
</li>
</ol>
<h3 id="2062-mandarin-protocol">20.6.2 中文评测标准 (Mandarin Protocol)</h3>
<ol>
<li><strong>空格移除</strong>: 中文无空格分词。<code>我 爱 你</code> → <code>我爱你</code>。</li>
<li><strong>全角转半角</strong>: 尤其针对英文和数字部分。</li>
<li><strong>繁转简</strong>: 绝大多数 Benchmarks（如 AISHELL）是简体的。如果模型输出繁体，必须转简后再评测。</li>
<li><strong>多音字容错 (高级)</strong>:
* Ref: <code>了解</code> (liao3 jie3)
* Hyp: <code>了结</code> (liao3 jie2)
* 这是错误。
* Ref: <code>什么</code>
* Hyp: <code>甚么</code>
* 如果在定义上视为同义词，可在评测脚本中加入<strong>同义词映射表</strong>，但在严格学术评测中通常<strong>不算对</strong>。</li>
</ol>
<h3 id="2063-code-switching">20.6.3 混合语种 (Code-Switching) 评测</h3>
<p>这是一个难点。</p>
<ul>
<li><strong>Tokenization</strong>: 英文按词（Word），中文按字（Char）。</li>
<li><strong>操作</strong>：</li>
<li>Ref: <code>我 love 你</code></li>
<li>Hyp: <code>我love你</code></li>
<li><strong>预处理</strong>：在汉字与英文之间插入空格。</li>
<li>Ref → <code>我 love 你</code> (Tokens: 我, love, 你)</li>
<li>Hyp → <code>我 love 你</code> (Tokens: 我, love, 你)</li>
<li>这样计算 MER (Mixed Error Rate) 才准确。</li>
</ul>
<hr />
<h2 id="207-gotchas">20.7 常见陷阱与调试技巧 (Gotchas)</h2>
<ol>
<li>
<p><strong>BOM 头噩梦</strong>:
* Windows 记事本保存的 UTF-8 文件开头可能有 <code>\ufeff</code> (Byte Order Mark)。
* Python 读取时如果不指定 <code>encoding='utf-8-sig'</code>，这会变成一个不可见字符 <code>ZWNBSP</code>，导致该文件第一句话的对齐全部错位。</p>
</li>
<li>
<p><strong>OpenCC 的副作用</strong>:
* 将“干货”转繁体时，可能变成“乾貨”（Dry goods）或“幹貨”（Doing stuff），取决于上下文。<code>s2twp</code> 通常能处理得比较好，但仍非 100% 准确。
* <strong>调试</strong>：抽样检查高频多义字（干、发、面、斗）的转换结果。</p>
</li>
<li>
<p><strong>JSONL 读取错误</strong>:
* 如果在 JSON 字符串内部包含未转义的控制符（如换行符 <code>\n</code>），标准 JSON Parser 会报错。
* <strong>Fix</strong>: 使用 <code>json.loads(line, strict=False)</code> 或在写入前做严格的字符串清洗。</p>
</li>
</ol>
<hr />
<h2 id="208">20.8 习题与实战</h2>
<h3 id="_1">基础题</h3>
<ol>
<li><strong>OpenCC 选型</strong>：如果你要利用大量来自香港的新闻语来训练一个通用的简体中文 ASR 模型，你应该使用哪个 OpenCC 配置文件？会对专有名词产生什么潜在影响？</li>
<li><strong>正则实战</strong>：写出一个正则逻辑，能够清洗掉字符串中所有的 <code>(鼓掌)</code>、<code>[笑声]</code> 等标注，但保留 <code>(IBM)</code>、<code>[NASA]</code> 这种包含英文大写的有效内容。</li>
<li><strong>Unicode</strong>：为什么在 macOS 上处理完的日语文本，传到 Linux 服务器上训练时，Tokenizer 可能会对“が”这个字产生不同的切分结果？</li>
</ol>
<h3 id="_2">挑战题</h3>
<ol start="4">
<li><strong>粤语标准化</strong>：设计一个决策树，用于处理包含 <code>"佢好happy既啫"</code> 这句话的数据。要求保留英文，统一粤语常用字，并去除无实义的语气助词（假设目标是生成简洁字幕）。</li>
<li><strong>混语评测</strong>：Ref 是 <code>"Project A 的进度是 50%"</code>，Hyp 是 <code>"project a 的进度是百分之五十"</code>。请设计一个归一化流程，使得 CER 计算结果合理（趋近于 0）。</li>
</ol>
<details>
<summary>点击展开参考答案</summary>
<ol>
<li><strong>OpenCC</strong>：应使用 <code>hk2s.json</code>。影响：香港特有的翻译名词（如“碧咸”-&gt;“贝克汉姆”通常不会自动发生，除非词典包含此映射）。如果不转换，“碧咸”会被当作普通词汇训练，导致在大陆语境下识别出 OOV。</li>
<li><strong>正则</strong>：<code>re.sub(r'[\[\(](?![A-Z0-9]+[\]\)])[\u4e00-\u9fa5]+[\]\)]', '', text)</code> (思路：负向先行断言，排除全大写/数字的内容)。</li>
<li><strong>Unicode</strong>：macOS 倾向于使用 NFD（分解形式，<code>ka</code>+<code>dakuten</code>），Linux/Windows 倾向于 NFC。如果 Tokenizer 是在 NFC 数据上训练的，遇到 NFD 的 <code>ka</code>+<code>dakuten</code> 会将其视为两个未知或独立的字符，导致 OOV 或对齐错误。</li>
<li>
<p><strong>粤语</strong>：
* Step 1: 英文保留 -&gt; "happy"
* Step 2: 正字法 -&gt; "既" 修正为 "嘅" (若视为语气词则在下一步删)
* Step 3: 语气词过滤 -&gt; 识别 "既(嘅)" 和 "啫" 为语气助词，删除。
* 结果 -&gt; "佢好 happy"</p>
</li>
<li>
<p><strong>混语评测</strong>：
* Step 1 (Lower): <code>Project A</code> -&gt; <code>project a</code>
* Step 2 (ITN): <code>50%</code> -&gt; <code>百分之五十</code> (或者反向：<code>百分之五十</code> -&gt; <code>50%</code>)。<strong>关键是统一</strong>。
* Step 3 (Space): 移除中文字间空格，保留英文周边空格。
* 结果：Ref 与 Hyp 字符序列一致。</p>
</li>
</ol>
</details>
            </article>
            
            <nav class="page-nav"><a href="chapter19.html" class="nav-link prev">← Chapter 19: 附录 A：TN / ITN 速查与工程实战手册</a><a href="chapter21.html" class="nav-link next">Chapter 21: 附录 C - 术语表、常见问答 (FAQ) 与进阶阅读 →</a></nav>
        </main>
    </div>
</body>
</html>