<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>Chapter 16: MLLM 时代：从 Speech Foundation Model 到“可对话的语音智能体”</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">ASR 与 Speaker Diarization 训练中文教程（多语种：中英及主要语种｜RNN → Conv+LSTM → MLLM）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 1: 任务全景：ASR 与 Diarization 的训练对象、边界与通用流水线</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 2: 工程与实验基线：环境、框架、分布式与可复现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 3: 数据与标注：采集、清洗、切分、对齐与许可</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 4: 文本规范化全家桶：TN / ITN / OpenCC / 混语与混脚本</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 5: 音频预处理与切分：VAD、重叠、对齐与波形级增广</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 6: 特征与前端：从 MFCC 到可学习前端，再到 SSL 表征</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 7: RNN 时代 ASR：从 LSTM/GRU 到 CTC/Attention（并讨论对 MLLM 的启示）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 8：Conv + LSTM 时代：CLDNN/CRDNN/TDNN-LSTM 与流式工程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 9: Transformer 自监督过渡：Conformer、Transducer 与 SSL 微调</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 10: Speaker Diarization 经典流水线：SAD + Embedding + Clustering + Resegmentation</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 11: 神经 Diarization 与端到端联合：EEND、TS-VAD、SA-ASR</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 12: 多语种与混语训练 (Multilingual & Code-Switching)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 13: 评测与误差分析——ASR (WER/CER/MER) 与 Diarization (DER/JER) 的细节陷阱</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 14: 开源工具链与训练配方：Kaldi / ESPnet / NeMo / WeNet / FunASR / Pyannote</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 15: 开源数据集大全：ASR / 多语种 / 会议 / 噪声 / Diarization</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Chapter 16: MLLM 时代：从 Speech Foundation Model 到“可对话的语音智能体”</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="chapter-16-mllm-speech-foundation-model">Chapter 16: MLLM 时代：从 Speech Foundation Model 到“可对话的语音智能体”</h1>
<blockquote>
<p><strong>本章摘要</strong>：
本章标志着从“专用模型”向“通用模型”的范式转移。我们将探讨如何将语音模态接入大语言模型（LLM），使其不仅具备 ASR（转写）能力，还具备语义理解（SQA）、指令遵循（Instruction Following）及多模态推理能力。
<strong>你将学到</strong>：</p>
<ol>
<li><strong>架构模式</strong>：级联 vs. 连续投影 vs. 离散 Token 的深度对比。</li>
<li><strong>训练配方</strong>：如何构建 Stage 1（对齐）与 Stage 2（指令微调）的数据。</li>
<li><strong>工程难题</strong>：如何解决幻觉、时间戳对齐以及长音频的 Context Window 爆炸。</li>
<li><strong>传承</strong>：传统 ASR 经验（CTC、规整化）在 MLLM 中的新角色。</li>
</ol>
</blockquote>
<hr />
<h2 id="161-asr">16.1 概念重构：ASR 不再是终点</h2>
<p>在进入技术细节前，必须更新一个核心认知：<strong>在 MLLM 时代，ASR 不再是一个独立的任务，而只是 LLM 的一种“读写能力”。</strong></p>
<p>| 维度 | 传统 ASR / End-to-End (Ch 7-9) | MLLM ASR (Speech-Language Model) |</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>传统 ASR / End-to-End (Ch 7-9)</th>
<th>MLLM ASR (Speech-Language Model)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>核心目标</strong></td>
<td>最小化 WER (Word Error Rate)</td>
<td>最大化语义理解与指令遵循</td>
</tr>
<tr>
<td><strong>输入输出</strong></td>
<td>Audio  Text</td>
<td>Audio + Text Prompt  Text Response</td>
</tr>
<tr>
<td><strong>建模范式</strong></td>
<td>$P(\text{Text}</td>
<td>\text{Audio})$</td>
</tr>
<tr>
<td><strong>世界知识</strong></td>
<td>几乎没有 (依赖外部 LM)</td>
<td>极其丰富 (内建于 LLM 参数中)</td>
</tr>
<tr>
<td><strong>典型代表</strong></td>
<td>Kaldi, Conformer-Transducer, Whisper</td>
<td>Qwen-Audio, SpeechGPT, GPT-4o</td>
</tr>
</tbody>
</table>
<h3 id="1611">16.1.1 关键术语辨析</h3>
<ul>
<li><strong>Speech Foundation Model (语音基座)</strong>: 如 <strong>Wav2Vec 2.0, HuBERT, WavLM, Whisper Encoder</strong>。它们是“耳朵”，负责把声音变成高维向量。它们不懂“这首歌很悲伤”，它们只知道声学特征。</li>
<li><strong>MLLM (多模态大模型)</strong>: 如 <strong>Qwen-Audio, LLaVA (Audio variant)</strong>。它们是“带耳朵的大脑”，能理解声音背后的含义。</li>
</ul>
<hr />
<h2 id="162">16.2 架构谱系：三大主流流派</h2>
<p>如何让本来只能看懂文字 Token 的 LLM “听懂”声音？目前工业界和学术界主要有三条技术路线。</p>
<h3 id="1621-a-cascade">16.2.1 流派 A：级联模式 (Cascade) —— “稳健的松耦合”</h3>
<p>最直观的方案：<strong>ASR Model + LLM</strong>。</p>
<ul>
<li><strong>工作流</strong>：Audio  Text Transcript  LLM  Result。</li>
<li><strong>优点</strong>：</li>
<li><strong>模块化</strong>：ASR 和 LLM 可以独立升级（例如 ASR 换成最新的 Paraformer，LLM 换成 Llama 3）。</li>
<li><strong>极低成本</strong>：不需要训练，全是推理。</li>
<li>
<p><strong>长音频友好</strong>：ASR 输出的文本 Token 数量远少于音频特征帧，且不受音频编码器显存限制。</p>
</li>
<li>
<p><strong>致命缺陷</strong>：</p>
</li>
<li><strong>信息丢失 (Information Loss)</strong>：这是级联模式的天花板。ASR 输出文本后，<strong>语气、情绪、停顿、说话人身份、背景噪声</strong>全部丢失。LLM 无法回答“说话人是不是在生气？”或“背景里有狗叫吗？”。</li>
<li><strong>误差传播</strong>：ASR 听错一个专有名词，LLM 大概率无法纠正。</li>
</ul>
<h3 id="1622-b-continuous-projection-sota">16.2.2 流派 B：连续特征投影 (Continuous Projection) —— “当前 SOTA 主流”</h3>
<p>这是目前（2024-2025）构建 MLLM ASR 的标准范式。核心思想是将音频编码器的输出特征，映射到 LLM 的 <strong>Text Embedding Space</strong>。</p>
<p><strong>架构图解 (ASCII)</strong>：</p>
<div class="codehilite"><pre><span></span><code>[ Audio Input (16kHz) ]
        |
+-----------------------+
|    Audio Encoder      |  &lt;-- 1. 耳朵 (通常冻结或小LR微调)
| (Whisper-Enc / WavLM) |      输出: 序列长度 T, 维度 D
+-----------------------+
        |
[ Acoustic Features ] (e.g., T=1500 for 30s audio)
        |
+-----------------------+
|  Modality Adaptor     |  &lt;-- 2. 桥梁 (训练重点!)
| (Projector/Connector) |      作用: 降维 + 长度压缩
+-----------------------+
        |
[ Audio Embeddings ] (e.g., T&#39;=300, 维度=LLM_Dim)
        |
        v
[ Text Embeddings ] &lt;---+
        |               |
+-----------------------+
|  LLM Backbone         |  &lt;-- 3. 大脑 (LoRA 或 全量微调)
| (Llama / Qwen / Vicuna)|
+-----------------------+
</code></pre></div>

<ul>
<li><strong>关键组件：Adaptor (适配器)</strong></li>
<li><strong>为什么需要它？</strong> 音频编码器的输出（如 1024维）与 LLM 的输入（如 4096维）不匹配；且音频帧率太高（50Hz），直接灌入会撑爆 LLM 的 Context Window。</li>
<li><strong>常用实现</strong>：
1. <strong>Linear Projector</strong>：简单的全连接层（Wav2LLM 早期做法）。
2. <strong>CNN / Downsampling</strong>：通过卷积步长（Stride）将 10ms/frame 压缩到 40-80ms/token。<strong>Rule of Thumb: 压缩率至少要 4x 到 8x。</strong>
3. <strong>Q-Former (BLIP style)</strong>：用一组 Learnable Queries 去“提取”音频中的关键信息，将不定长音频压缩为定长 Token（如 64 个 Token）。</li>
</ul>
<h3 id="1623-c-token-discrete-tokenization">16.2.3 流派 C：离散 Token (Discrete Tokenization) —— “迈向原生统一”</h3>
<p>试图将音频完全“文本化”。</p>
<ul>
<li><strong>核心</strong>：使用 <strong>Neural Audio Codec</strong> (如 EnCodec, SoundStream) 将波形量化为离散的 Codebook ID（如 0-1023）。</li>
<li><strong>做法</strong>：扩充 LLM 的词表，加入音频 Token。</li>
<li>
<p>词表 = <code>{Text Tokens} U {Audio Tokens}</code></p>
</li>
<li>
<p><strong>优势</strong>：LLM 可以像生成文本一样生成音频（Speech-to-Speech）。</p>
</li>
<li><strong>劣势</strong>：<strong>信息密度极低</strong>。1秒音频可能对应 25-75 个 Audio Tokens，而对应文本可能只有 2-3 个 Tokens。这对 LLM 的长序列建模能力是极大的考验。</li>
</ul>
<hr />
<h2 id="163-feature-alignment-instruction-tuning">16.3 训练范式：Feature Alignment 与 Instruction Tuning</h2>
<p>训练一个 MLLM ASR 通常分为两个阶段。忽略任何一个阶段都会导致模型不可用。</p>
<h3 id="1631-stage-1-pre-training-feature-alignment">16.3.1 Stage 1: 特征对齐 (Pre-training / Feature Alignment)</h3>
<ul>
<li><strong>目标</strong>：让 Adaptor 学会“翻译”。此时 LLM 不懂指令，只懂描述。</li>
<li><strong>数据</strong>：大规模 <code>&lt;Audio, Text&gt;</code> 配对数据（如 WenetSpeech, LibriSpeech）。</li>
<li><strong>训练策略</strong>：</li>
<li>冻结 Audio Encoder。</li>
<li>冻结 LLM Backbone。</li>
<li>
<p><strong>只训练 Adaptor</strong>。</p>
</li>
<li>
<p><strong>Loss</strong>：Next Token Prediction (NTP)。</p>
</li>
<li><strong>Prompt 模板</strong>：通常很简单如 <code>Audio content: &lt;speech_text&gt;</code>。</li>
</ul>
<h3 id="1632-stage-2-instruction-tuning-sft">16.3.2 Stage 2: 指令微调 (Instruction Tuning / SFT)</h3>
<p>这是赋予模型“智能”和“多任务能力”的关键。</p>
<ul>
<li><strong>训练策略</strong>：</li>
<li>解冻 LLM（全量或 LoRA）。</li>
<li>继续训练 Adaptor。</li>
<li>(可选) 解冻 Audio Encoder 的最后几层。</li>
</ul>
<h4 id="sft-data-recipe">核心：SFT 数据配比 (Data Recipe)</h4>
<p>很多项目失败是因为只用了 ASR 数据。<strong>Rule of Thumb</strong> 的数据配比如下：</p>
<p>| 任务类型 | 占比建议 | 作用 | 示例 Prompt |</p>
<table>
<thead>
<tr>
<th>任务类型</th>
<th>占比建议</th>
<th>作用</th>
<th>示例 Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ASR (转写)</strong></td>
<td>40% - 50%</td>
<td>保持听写的基本功</td>
<td>"Please transcribe the audio accurately."</td>
</tr>
<tr>
<td><strong>SQA (语音问答)</strong></td>
<td>20% - 30%</td>
<td>建立从声学到语义的理解</td>
<td>"Based on the audio, what is the speaker's attitude?"</td>
</tr>
<tr>
<td><strong>Summarization</strong></td>
<td>10%</td>
<td>锻炼长上下文归纳能力</td>
<td>"Summarize the key points of this speech."</td>
</tr>
<tr>
<td><strong>Text-only (纯文本)</strong></td>
<td>10% - 20%</td>
<td><strong>防止灾难性遗忘 (Catastrophic Forgetting)</strong></td>
<td>(常规对话数据)</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Gotcha</strong>: 如果不加纯本数据，微调后的模型可能会丧失原本的逻辑推理能力，甚至连简单的文本对话都做不好。</p>
</blockquote>
<hr />
<h2 id="164-mllm-asr">16.4 MLLM ASR 的特有挑战与工程解法</h2>
<h3 id="1641-hallucination">16.4.1 幻觉 (Hallucination) 与“过度生成”</h3>
<p>MLLM 最典型的问题是“听完一段静音，自己编造了一段话”或者“把背景歌词当成了说话内容”。</p>
<ul>
<li><strong>原因</strong>：LLM 本质是概率预测，它倾向于生成“通顺”的句子，而不是“忠实”的句子。</li>
<li><strong>解法</strong>：
1. <strong>System Prompt 约束</strong>：强制加入 <code>"Do not generate content not present in the audio."</code>
2. <strong>数据清洗</strong>：剔除静音片段对应有文本的脏数据。
3. <strong>Suppression</strong>：在 Inference 阶段，如果检测到音频 VAD 为静音，直接截断 LLM 的生成。</li>
</ul>
<h3 id="1642-timestamp">16.4.2 时间戳 (Timestamp) 预测</h3>
<p>传统 ASR 会自然给出每个词的时间。LLM 只有一个输出序列，如何给时间戳？</p>
<ul>
<li><strong>Whisper 范式</strong>：将时间量化为特殊 Token。例如将 30s 音频分为 1500 份（0.02s 精度），定义 <code>&lt;|0.00|&gt;</code>, <code>&lt;|0.02|&gt;</code> ... <code>&lt;|30.00|&gt;</code> 等 1500 个 Token。</li>
<li><strong>Interleaved 输出格式</strong>：</li>
<li>训练目标：<code>&lt;|0.00|&gt; Hello &lt;|0.50|&gt; world &lt;|1.00|&gt;</code></li>
<li>这意味着你的训练数据必须经过<strong>强对齐（Force Alignment, see Chapter 3）</strong>处理。</li>
</ul>
<h3 id="1643-context-explosion">16.4.3 上下文窗口爆炸 (Context Explosion)</h3>
<ul>
<li><strong>问题</strong>：即使经过 Adapter 4x 压缩，1分钟的音频也可能产生 750-1500 个 Audio Embeddings。多轮对话几下就爆了显存。</li>
<li><strong>工程解法</strong>：</li>
<li><strong>Window Attention</strong>：只让 LLM 关注最近的 N 秒音频。</li>
<li><strong>Dynamic Resolution</strong>：对静音段使用高压缩率，对语音段使用低压缩率（需要 VAD 辅助）。</li>
<li><strong>早融合 (Early Fusion)</strong>：在 Encoder 阶段就做更激进的 Stacking（如 CIF - Continuous Integrate-and-Fire）。</li>
</ul>
<hr />
<h2 id="165-mllm-bridge">16.5 传统时代对 MLLM 的启示 (Bridge)</h2>
<p>不要扔掉你的 Kaldi 和 WeNet 知识，它们在 MLLM 时代有新用途：</p>
<ol>
<li>
<p><strong>CTC 的“辅助对齐”作用</strong>：
* 纯 Autoregressive (AR) 生成很容易出现“丢词”或“重复”。
* <strong>混合 Loss</strong>：在 Adapter 层加一个小的 CTC Head 进行辅助训练，可以强迫 Audio Encoder 学到更好的对齐信息，减少 LLM 的幻觉。</p>
</li>
<li>
<p><strong>规整化 (TN) 的回归 (Ref Chapter 4)</strong>：
* MLLM 输出的数字格式极其不可控（"three" vs "3"）。
* <strong>Tool Use 思想</strong>：不要强求 LLM 输出完美格式。让 LLM 输出 Raw Text，然后用传统的 ITN 脚本（WFST）进行后处理。</p>
</li>
<li>
<p><strong>Diarization 作为 Prompt (Ref Chapter 10)</strong>：
* MLLM 很难自己区分说话人。
* <strong>Pipeline</strong>：先跑 Pyannote 得到 <code>Timeline string</code>，注入到 Prompt：</p>
<blockquote>
<p>"Input Audio: [Audio Embedding]
Speaker Info: 0-5s Speaker A; 5-10s Speaker B.
Task: Transcribe distinct speakers."</p>
</blockquote>
</li>
</ol>
<hr />
<h2 id="166">16.6 本章小结</h2>
<ol>
<li><strong>范式转移</strong>：从“ASR 模型”转向“具备听觉的 LLM”。</li>
<li><strong>架构选择</strong>：<strong>连续投影 (Projector)</strong> 是当前性价比最高的工业级方案。</li>
<li><strong>数据决定成败</strong>：没有 Instruction Tuning，就没有智能；没有纯文本混合，就会变笨。</li>
<li><strong>对齐难题</strong>：通过压缩 Adapter 和量化时间戳 Token，解决模态不匹配问题。</li>
</ol>
<hr />
<h2 id="167-exercises">16.7 练习题 (Exercises)</h2>
<h3 id="_1">基础题</h3>
<ol>
<li><strong>维度匹配</strong>：假设 Audio Encoder 输出维度是 768，LLM 输入维度是 4096。请写出一个简单的 PyTorch <code>Linear</code> 层定义来实现 Projector。</li>
<li><strong>数据格式</strong>：为了训练一个能识别“会议摘要”的 MLLM，你需要构造一条 JSONL 数据。请补全以下空白：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="p">{</span><span class="nt">&quot;audio&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;meeting_01.wav&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;instruction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;______&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;output&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;______&quot;</span><span class="p">}</span>
</code></pre></div>

<ol start="3">
<li><strong>压缩率计算</strong>：音频帧率为 50Hz（即 20ms 一帧）。如果 Adapter 使用了两个 stride=2 的 CNN 层，最终进入 LLM 的 Token 帧率是多少？10秒音频会变成多少个 Token？</li>
</ol>
<h3 id="_2">挑战题</h3>
<ol start="4">
<li><strong>长音频推理设计</strong>：你的显卡显存只能容纳 30秒的音频 Embeddings，但用户上传了 1 小时的录音。请设计一个利用 RAG（Retrieval-Augmented Generation）或 Sliding Window 的方案，让 MLLM 能够回答关于这 1 小时音频内容的提问。</li>
<li><strong>Code-switch 漂移</strong>：在微调中英混合数据时，发现 LLM 总是把英文部分翻译成中文（例如听到 "I agree" 输出 "我同意"）。除了修改 Prompt，你还能从 Loss Function 或数据构造层面做什么调整？</li>
</ol>
<details>
<summary><b>点击查看答案与提示</b></summary>
<ol>
<li><strong>维度匹配</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
<span class="c1"># 进阶：通常会加 Activation 和 LayerNorm</span>
<span class="c1"># projector = nn.Sequential(nn.Linear(768, 4096), nn.GELU(), nn.LayerNorm(4096))</span>
</code></pre></div>

<ol start="2">
<li><strong>数据格式</strong></li>
</ol>
<ul>
<li><code>instruction</code>: "Please summarize the key decisions made in this meeting audio."</li>
<li><code>output</code>: "The meeting discussed Q3 budget. Decision 1: Approve marketing spend..." (摘要文本)</li>
</ul>
<ol start="3">
<li><strong>压缩率计算</strong></li>
</ol>
<ul>
<li><strong>总下采样率</strong>： 倍。</li>
<li><strong>新帧率</strong>：。</li>
<li><strong>Token 数</strong>：。</li>
</ul>
<ol start="4">
<li><strong>长音频推理设计</strong></li>
</ol>
<ul>
<li>
<p><strong>方案</strong>：<strong>ASR + Chunked Summarization + RAG</strong>。
1. 先用级联 ASR 快速转写全文（低成本）。
2. 将文本切块（Chunking），每块 500 字。
3. 对每块做 Embedding 存入向量库（Vector DB）。
4. 当用户提问时，检索相关文本块。
5. 将检索到的文本块 + 对应时间段的<strong>原始音频片段</strong>（作为 Audio Prompt）一起喂给 MLLM。</p>
</li>
<li>
<p><em>Prompt</em>: "Here is the text context: [...]. Here is the audio snippet: [Audio Emb]. Answer the user question."</p>
</li>
</ul>
<ol start="5">
<li><strong>Code-switch 漂移</strong></li>
</ol>
<ul>
<li><strong>数据构造</strong>：构造 <strong>Negative Constraints</strong>。</li>
<li>Input: (Audio: "I agree")</li>
<li>Target: "I agree"</li>
<li>
<p>如果模型输出 "我同意"，在训练时虽然不能直接计算 Loss，但可以构造对比学习样本。</p>
</li>
<li>
<p><strong>更实用的方法</strong>：<strong>Token Level Loss Masking</strong>。确保训练数据中的英文部分对应的 Token ID 是英文字符，如果模型预测了中文字符 ID，Loss 会非常大。</p>
</li>
<li><strong>Auxiliary Task</strong>：增加一个 Language ID (LID) 预测任务头，强迫 Encoder 区分语种。</li>
</ul>
</details>
<hr />
<h2 id="168-gotchas">16.8 常见陷阱与错误 (Gotchas)</h2>
<h3 id="1-adapter-vs">1. Adapter 训练不足 vs 过拟合</h3>
<ul>
<li><strong>陷阱</strong>：很多人在 Stage 1（对齐）只跑了几个 Epoch，Loss 还没收敛就开始 Stage 2。</li>
<li><strong>后果</strong>：模型听力极差。Stage 1 必须像预训练一样跑足量数据（Recommendation: 至少 5k-10k 小时音频）。</li>
<li><strong>反向陷阱</strong>：Adapter 参数量太大（如 &gt;200M），导致在小数据集上过拟合，对新说话人泛化能力差。<strong>Adapter 应当是轻量级的（10M-50M params）</strong>。</li>
</ul>
<h3 id="2-end-of-speech-token">2. 忽视了 <code>End-of-Speech</code> Token</h3>
<ul>
<li><strong>现象</strong>：模型准确转写了内容，但最后一直不停止，输出乱码或重复。</li>
<li><strong>原因</strong>：音频结束没有明确的边界信号。</li>
<li><strong>Fix</strong>：在训练数据的 Audio Token 序列末尾显式加入 <code>&lt;|audio_end|&gt;</code> 特殊 Token，并计算其 Loss。</li>
</ul>
<h3 id="3-whisper-encoder-bug">3. Whisper Encoder 的“静音”Bug</h3>
<ul>
<li><strong>现象</strong>：如果使用 Whisper 作为 Encoder，它在纯静音片段有时会输出奇怪的幻觉文本。</li>
<li><strong>Fix</strong>：在送入 MLLM 前，先过一个轻量级的 <strong>VAD (Voice Activity Detection)</strong>。如果是静音，直接丢弃或替换为 <code>&lt;|silence|&gt;</code> Token，不要让 Audio Encoder 瞎猜。</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter15.html" class="nav-link prev">← Chapter 15: 开源数据集大全：ASR / 多语种 / 会议 / 噪声 / Diarization</a><a href="CLAUDE.html" class="nav-link next">Untitled →</a></nav>
        </main>
    </div>
</body>
</html>